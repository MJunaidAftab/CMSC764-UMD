{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: **Muhammad Junaid Aftab**\n",
    "UID:  **117396188**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3:  Gradients \n",
    "\n",
    "For this assignment, you'll need the results of Homework 2.  We'll import your homework solutions.  Put the file `notebook_importer.py` into the directory with your Homework 3 file.  Then run the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from hmwk2_more_linalg_sols.ipynb\n",
      "Recovery error (clean) = 2.71e-06\n",
      "measurement error = 1.79e-07\n",
      "Recovery error (noisy) = 178.930\n",
      "Adjoint Test Failed, rel_diff = 0.99995\n",
      "Adjoint Test Passed, rel_diff = 3.0552317883369726e-16\n",
      "Adjoint Test Passed, rel_diff = 5.300242344563257e-16\n",
      "Tests PASSED! You're on your way to understanding linear operators!\n",
      "TESTS PASSED!  YOU ROCK!\n",
      "Adjoint Test Passed, rel_diff = 2.542717181643103e-16\n",
      "Adjoint Test Passed, rel_diff = 1.0633148430239133e-16\n",
      "Adjoint Test Passed, rel_diff = 1.0355040823076723e-16\n",
      "Unit tests PASSED!  You're getting really good at this!\n",
      "Horizontal error =  8.614337865368981e-14\n",
      "Vertical error =  8.54345073528387e-14\n",
      "Tests PASSED!  Wow - you're a linear algebra GENIUS!\n"
     ]
    }
   ],
   "source": [
    "#import notebook_importer\n",
    "import io\n",
    "import import_ipynb\n",
    "from hmwk2_more_linalg_sols import grad2d, divergence2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, setup the environment with a bunch of functions you'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the environment - do not modify this cell, but run it before anything else\n",
    "import numpy as np\n",
    "from numpy import sqrt, sum, abs, max, maximum, logspace, exp, log, log10, zeros\n",
    "from numpy.random import randn, normal, choice\n",
    "from numpy.linalg import norm\n",
    "import scipy\n",
    "from scipy.linalg import orth\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "def good_job(path):\n",
    "    #a = plt.imread(io.BytesIO(f.read()))\n",
    "    a = plt.imread(urllib.request.urlopen(path))\n",
    "    fig = plt.imshow(a)\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful tools\n",
    "Run the following code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def buildmat(m,n,cond_number):\n",
    "    \"\"\"Build an mxn matrix with condition number cond.\"\"\"\n",
    "    if m<=n:\n",
    "        U = randn(m,m);\n",
    "        U = orth(U);\n",
    "        Vt = randn(n, m);\n",
    "        Vt = orth(Vt).T;\n",
    "        S = 1/logspace(0,log10(cond_number),num=m);\n",
    "        return (U*S[:,None]).dot(Vt)\n",
    "    else:\n",
    "        return buildmat(n,m,cond_number).T\n",
    "    \n",
    "def create_classification_problem(num_data, num_features, cond_number):\n",
    "    \"\"\"Build a simple classification problem.\"\"\"\n",
    "    X = buildmat(num_data, num_features, cond_number)\n",
    "    # The linear dividing line between the classes\n",
    "    w =  randn(num_features,1)\n",
    "    # create labels\n",
    "    prods = X@w\n",
    "    y = np.sign(prods)\n",
    "    #  mess up the labels on 10% of data\n",
    "    flip = choice(range(num_data),int(num_data/10))\n",
    "    y[flip] = -y[flip]\n",
    "    #  return result\n",
    "    return X,y\n",
    "# Visualize this classification problem\n",
    "X,y = create_classification_problem(100, 2, 5)\n",
    "y = y.ravel()\n",
    "plt.scatter(X[:,0][y>0], X[:,1][y>0], color='r')\n",
    "plt.scatter(X[:,0][y<0], X[:,1][y<0], color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Gradient checker\n",
    "Write a method for testing whether the function `grad` generates the gradient of `f`.  Do this by generating a random perturbation $\\delta$ and then testing whether\n",
    "  $$\\frac{f(x+\\delta) -f(x-\\delta)}{2} \\approx  \\delta^\\top \\nabla f(x).$$\n",
    "  The method should generate a random Gaussian $\\delta$, check the gradient condition, and then replace $\\delta \\gets \\delta/10.$  Do this for 10 different orders of magnitude of $\\delta.$ For each order, compute the **relative** error between the left and right side of the above equation.  Finally, print the minimum relative error achieved.  All of your print statement should label what they are printing, i.e., don't output random/unlabeled numbers to the console.  The method returns `True` if the gradient is correct up to 1 part in 1 million, and `False` otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient(f, grad, x):\n",
    "    \n",
    "    n_variables_r, n_variables_c   = np.shape(x)[0], np.shape(x)[1]\n",
    "    n_iterations = 10\n",
    "    min_error = 1e10\n",
    "    \n",
    "    delta =  randn(n_variables_r,n_variables_c)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "    \n",
    "        lhs = (f(x+delta) - f(x-delta))/2 \n",
    "        rhs = np.vdot(delta,grad(x))\n",
    "        error = abs(lhs - rhs)\n",
    "        rel_error = error/abs(rhs)\n",
    "        \n",
    "        if rel_error < min_error:\n",
    "            \n",
    "            min_error = rel_error\n",
    "            \n",
    "        delta = delta/10\n",
    "        \n",
    "    print('The minimum relative error achieved is ', min_error)\n",
    "        \n",
    "    return min_error < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, run this unit test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum relative error achieved is  [[0.]]\n",
      "The minimum relative error achieved is  [[1.30611754e-05]]\n",
      "Tests passed!  Your gradient checker is like totally awesome!\n"
     ]
    }
   ],
   "source": [
    "# This test should pass\n",
    "f    = lambda x: 0.5*x.T@x\n",
    "grad = lambda x: x\n",
    "x = randn(10,1)\n",
    "did_pass = check_gradient(f, grad, x)\n",
    "assert did_pass, \"Test should have passed, but failed\"\n",
    "# This test should fail\n",
    "grad = lambda x: x+1e-5\n",
    "did_pass = check_gradient(f, grad, x)\n",
    "assert not did_pass, \"Test should have failed, but passed\"\n",
    "\n",
    "print(\"Tests passed!  Your gradient checker is like totally awesome!\")\n",
    "#good_job(\"https://www.cs.umd.edu/~tomg/img/important_memes/congrats_work.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Logistic regression\n",
    "Write a routine that evaluates the logistic loss function\n",
    "$$L(z) = \\sum_i \\ln(1+e^{-z_i}).$$\n",
    "Then, write a routine for evaluating the logistic regression objective function\n",
    "$$f(w) = L(YXw)$$\n",
    "where $Y$ is a diagonal matrix of labels, $X$ is matrix of training data, and $w$ is the slope vector.\n",
    "\n",
    "Your implementation must satisfy these criteria:\n",
    "\n",
    "-  You cannot use ANY `for` loops, or any other loops.\n",
    "-  You may not explicitly form the matrix $Y.$ You may only store the vector $y.$ \n",
    "-  You can **never** exponentiate a positive number.  In other words, you can't evaluate $e^z$ for $z>0.$   Computing $e^z$ is dangerous because you get `NaN` when $z$ is big, and this will crash your code. \n",
    "-  Your `logistic_loss` routine can be at most 6 (short) lines of code (excluding return line and signature).\n",
    "-  Your `logreg_objective` can be at most 2 (short) lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def logistic_loss(z):\n",
    "    \"\"\"Return sum(log(1+exp(-z))). Your implementation can NEVER exponentiate a positive number.  No for loops.\"\"\"\n",
    "    loss, z_pos, z_neg = 0, np.where(z > 0, z, 0), np.where(z < 0, z, 0)\n",
    "    term1 = log(1 + exp(-z_pos[z_pos != 0]))\n",
    "    term2 = log(1 + exp(z_neg[z_neg != 0])) - z_neg[z_neg != 0]\n",
    "    loss = loss + np.sum(term1)\n",
    "    loss = loss + np.sum(term2)\n",
    "    return loss \n",
    "            \n",
    "def logreg_objective(w,X,y):\n",
    "    \"\"\"Evaluate the logistic regression loss function on the data and labels, where the rows of D contain \n",
    "    feature vectors, and y is a 1D vector of +1/-1 labels.\"\"\"\n",
    "    vec = y*(np.matmul(X,w))\n",
    "    return logistic_loss(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now run this unit test...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED!  Your logistic loss works!\n"
     ]
    }
   ],
   "source": [
    "# Test logistic_loss using a for loop implementation\n",
    "X, Y = create_classification_problem(100, 10, 10)\n",
    "w = randn(10,1)\n",
    "output = logreg_objective(w,X,Y)\n",
    "loss = 0\n",
    "for x,y in zip(X,Y):\n",
    "    z = y*(x@w)\n",
    "    loss += log(1+exp(-z))\n",
    "assert abs(output-loss)<1e-10, 'Test FAILED:  your loss is incorrect'\n",
    "\n",
    "output = logreg_objective(1e9*w,X,Y)\n",
    "assert np.isfinite(output), \"Test FAILED:  Your routine is not numerically stable.  Maybe you exponentiated a positive number?\"\n",
    "\n",
    "print('Test PASSED!  Your logistic loss works!')\n",
    "#good_job(\"https://www.cs.umd.edu/~tomg/img/important_memes/dog.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now write a routine that produces the gradient of the logistic loss, and logistic objective.**  No `for` loops.  These routines should be short.  Remember, don't exponentiate a positive number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss_grad(z):\n",
    "    \"\"\"Gradient of logistic loss\"\"\"\n",
    "    z_pos, z_neg = np.where(z > 0, z, 0), np.where(z < 0, z, 0)\n",
    "    term1 = - 1/(1 + exp(z_neg))\n",
    "    term2 = - exp(-z_pos)/(1 + exp(-z_pos))\n",
    "    loss_grad = term1 + term2 + 0.5\n",
    "    return loss_grad\n",
    "\n",
    "def logreg_objective_grad(w,X,y):\n",
    "    vec = y*(np.matmul(X,w))\n",
    "    grad_vec = logistic_loss_grad(vec).T\n",
    "    return np.matmul(grad_vec,(y*X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this routine to check that your gradients are correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum relative error achieved is  1.0875367718264563e-11\n",
      "The minimum relative error achieved is  2.7524389622683573e-11\n",
      "Tests passed!  Your logistic gradients are perfect!\n"
     ]
    }
   ],
   "source": [
    "# Test the logistic gradient accuracy\n",
    "z = randn(10,1)\n",
    "did_pass = check_gradient(logistic_loss, logistic_loss_grad, z)\n",
    "assert did_pass, \"Incorrect gradient for logistic_loss\"\n",
    "\n",
    "# Test the logistic gradient stability\n",
    "lossgrad = logistic_loss_grad(z*1e9)\n",
    "assert np.alltrue(np.isfinite(lossgrad)), \"FAILED: Logistic gradient is unstable.  Did you exponentiate a positive number?\"\n",
    "\n",
    "# Test the logreg objective gradient accuracy\n",
    "X, y = create_classification_problem(100, 10, 10)\n",
    "f = lambda w: logreg_objective(w,X,y)\n",
    "grad = lambda w: logreg_objective_grad(w,X,y)\n",
    "w = randn(10,1)\n",
    "did_pass = check_gradient(f, grad, w)\n",
    "assert did_pass, \"Incorrect gradient for logistic_loss\"\n",
    "\n",
    "print(\"Tests passed!  Your logistic gradients are perfect!\")\n",
    "#good_job(\"https://www.cs.umd.edu/~tomg/img/important_memes/hey.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - TV\n",
    "Implement the total-variation denoising objective\n",
    "$$\\mu |\\nabla_d x| + \\frac{1}{2}\\|x-b\\|^2$$\n",
    "where $\\mu$ is an arbitrary scalar, $\\nabla_d$ denotes the discrete 2d gradient, and $b$ is a noisy image.  The symbol $\\nabla_d$ is the discrete 2D gradient operator, and produces all the first-order differences between adjacent pixels.\n",
    "\n",
    "Because the $\\ell_1$ norm is non-differentiable, replace it with it's hyperbolic regularization $|z| \\approx \\sum_i\\sqrt{z_i^2+\\epsilon^2}.$ \n",
    "\n",
    "You must use the grad2d routine from Homework 2.  It was imported by the first code cell in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(z, eps=.01):\n",
    "    \"\"\"The hyperbolic approximation to L1\"\"\"\n",
    "    u = np.sum(sqrt(z*z + eps*eps))\n",
    "    return u\n",
    "\n",
    "def tv_denoise_objective(x,mu,b):\n",
    "    u = mu*h(grad2d(x), eps=.01) + np.sum((x-b)*(x-b))/2\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now run this routine to check that your method runs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED!\n"
     ]
    }
   ],
   "source": [
    "# Create a simple test image\n",
    "b = zeros((100,100))\n",
    "b[25:75,25:75] = 2\n",
    "x = zeros((100,100))\n",
    "mu = 1\n",
    "# Evaluate the loss\n",
    "tvobj = tv_denoise_objective(x,mu,b)\n",
    "assert tvobj==5200.0, \"FAILED!  Your TV objective is incorrect.\"\n",
    "print('Test PASSED!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now implement these gradient routines.**  Use the divergence2d method from Homework 2. Each routine should be a few lines of code (in my solution each routine is only 1 line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_grad(z, eps=.01):\n",
    "    \"\"\"The gradient of h\"\"\"\n",
    "    u = z/sqrt(z*z + eps*eps)\n",
    "    return u\n",
    "\n",
    "def tv_denoise_grad(x,mu,b):\n",
    "    \"\"\"The gradient of the TV objective\"\"\"\n",
    "    u = mu*divergence2d(h_grad(grad2d(x), eps=.01)) + (x-b) \n",
    "    return u\n",
    "#*divergence2d(grad2d(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now run these unit tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum relative error achieved is  2.3419927316631026e-07\n"
     ]
    }
   ],
   "source": [
    "b = randn(100,100)\n",
    "x = randn(100,100)\n",
    "mu = 1\n",
    "f = lambda x: tv_denoise_objective(x,mu,b)\n",
    "grad = lambda x: tv_denoise_grad(x,mu,b)\n",
    "did_pass = check_gradient(f,grad,x)\n",
    "assert did_pass, \"FAILED:  Your gradient operator is no good\"\n",
    "\n",
    "#good_job(\"https://www.cs.umd.edu/~tomg/img/important_memes/you_rock.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
