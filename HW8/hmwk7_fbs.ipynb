{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: **Muhammad Junaid Aftab**\n",
    "UID:  **117396188**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7:  Splitting Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import *\n",
    "import numpy as np\n",
    "from numpy import sqrt, sum, abs, sign, max, maximum, minimum, logspace, exp, log, log10, zeros\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import randn, rand, normal, randint\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "def good_job(path):\n",
    "    a = plt.imread(urllib.request.urlopen(path))\n",
    "    fig = plt.imshow(a)\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: FBS\n",
    "Write a method that uses FBS to solve a general problem of the form \n",
    "$$\\min \\quad f(x)+g(x),$$\n",
    "where $f$ is smooth and $g$ is simple.\n",
    "You can do this by building on your gradient descent code from homework 2.  \n",
    "The argument $f$ computes the scalar-valued function $f(x).$  The argument `gradf` is a function handle that computes the gradient of $f.$  This means that\n",
    " $$ \\mathtt{gradf}(x) = \\nabla f(x).$$ \n",
    "  The argument `proxg` is a function handle that computes the proximal operator of $g$ with stepsize $\\tau.$  This means that \n",
    "  $$proxg(z,\\tau) = \\arg \\min_x  g(x) + \\frac{1}{2\\tau}\\|x-z\\|^2.$$\n",
    "  Your method should start by estimating the initial stepsize $\\tau$ using the Lipschitz constant for the gradient of $f.$  You already did this in homework 5 in your gradient descent method. The method should then perform an iteration of FBS, and use a backtracking line search until the following condition is met:\n",
    "      $$f(x^{k+1}) \\le f(x^k)+\\langle x^{k+1}-x^k, \\nabla f(x^k) \\rangle + \\frac{1}{2\\tau}\\|x^k-x^{k+1}\\|^2.$$\n",
    "\n",
    "Your method should terminate when the residual\n",
    "   $\\frac{1}{\\tau}\\|x^{k+1}-x^k\\|$\n",
    "   is \"small\" according to some reasonable criteria.  Formulas for the line search and residuals can be found in the paper \"A field guide to forward backward splitting with a FASTA implementation,\" (the line search condition is discussed in section 4.4, and formulas for the residuals are in section 4.6)  and also in the lecture slides.\n",
    "   \n",
    "  Your method should return an array containing the solution to the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  fbs(f, gradf, proxg, x0, max_iters = 10000, tol=1e-6):\n",
    "    \n",
    "    tau = 10/(estimate_lipschitz(f, x0))\n",
    "    xk = x0\n",
    "    d = -gradf(xk)\n",
    "    \n",
    "    k = 0\n",
    "    Boolean = True\n",
    "    \n",
    "    while k <= max_iters and Boolean == True:\n",
    "        \n",
    "        yk1 = xk + tau*d\n",
    "        xk1 = proxg(yk1,tau)\n",
    "        \n",
    "        while f(xk1) > f(xk) + np.vdot(xk1 - xk,-d) + (1/(2*tau))*norm(xk - xk1):\n",
    "            \n",
    "            tau = tau/2\n",
    "            xk1 = proxg(yk1,tau)\n",
    "        \n",
    "        #if norm(xk - xk1) < tau*tol:\n",
    "            \n",
    "        #    Boolean = False\n",
    "        \n",
    "        xk = xk1\n",
    "        d = -gradf(xk)\n",
    "          \n",
    "        k = k + 1\n",
    "    \n",
    "    return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, run this unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your FBS solver worked!\n"
     ]
    }
   ],
   "source": [
    "# Minimizer f+g where f(x)=.5(x+2)^2, g(x)=.5(x)^2\n",
    "f = lambda x: 0.5*(x-2)**2\n",
    "gradf = lambda x: x-2\n",
    "g = lambda x: 0.5*x**2\n",
    "proxg = lambda z,t: z/(1+t)\n",
    "x0 = np.array(5)\n",
    "\n",
    "x = fbs(f,gradf,proxg,x0)\n",
    "assert abs(x-1)<1e-4, \"Your solution is not accurate enough!\"\n",
    "print('Your FBS solver worked!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's cook up a sparse least squares problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this block!\n",
    "A = randn(100,200)\n",
    "x_true = zeros((200,1))\n",
    "x_true[0:10] = 1\n",
    "x_true[10:] = randn(190,1)*.01\n",
    "b = A@x_true + 0.2*randn(100,1)  # Add roughly 10% noise\n",
    "mu = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use your FBS solver on the problem\n",
    "$$\\arg \\min \\quad  \\mu|x| + \\frac{1}{2}\\|Ax-b\\|^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out the implementations of these methods. I suggest using lambda functions to keep it short, \n",
    "# but you can expand them into multi-line functions if you need.\n",
    "f = lambda x: 0.5*norm(A@x - b)**2\n",
    "gradf = lambda x: np.transpose(A)@(A@x-b)\n",
    "g = lambda x: mu*np.linalg.norm(x,1)\n",
    "proxg = lambda z,t: np.sign(z)*np.fmax(np.abs(z) - mu*t, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, run this method to call the solver and test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PASSED! Wow! You are so in the zone right now!\n"
     ]
    }
   ],
   "source": [
    "# Do not modify this block!\n",
    "x0 = zeros((200,1))\n",
    "x = fbs(f,gradf,proxg,x0, tol=1e-8) # Note: I use super high accuracy here so that my unit best below works\n",
    "\n",
    "# Test that your solution satisfies the optimality condition for the problem\n",
    "final_grad = gradf(x)\n",
    "assert max(abs(final_grad[x==0])) <= mu+1e-4, \"Your solution is incorrect\"\n",
    "assert norm(final_grad[x!=0] + mu*sign(x[x!=0]))/norm(final_grad[x!=0]) < 1e-4,  \"Your solution is incorrect\"\n",
    "print('TEST PASSED! Wow! You are so in the zone right now!')\n",
    "#good_job(\"https://www.cs.umd.edu/~tomg/img/important_memes/otter.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ya know what...I changed my mind. I actually want you to solve this problem:\n",
    "$$\\arg \\min \\quad  \\frac{1}{2}\\|Ax-b\\|^2$$ \n",
    "$$\\text{subject to } x \\ge 0.$$  \n",
    "Update your \"g\" function accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = lambda x: 0  # Do not change this line\n",
    "def proxh(z,t): \n",
    "    return np.fmax(z,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool. Now call the solver and test results on the new problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PASSED! Wow! You are so in the zone right now!\n"
     ]
    }
   ],
   "source": [
    "# Do not modify this block!\n",
    "x0 = zeros((200,1))\n",
    "x = fbs(f, gradf, proxh, x0, tol=1e-8) # Note: I use super high accuracy here so that my unit best below works\n",
    "\n",
    "# Test that your solution satisfies the optimality condition for the problem\n",
    "final_grad = gradf(x)\n",
    "assert min(x) >= 0, \"Your solution does not satisfy the constraint\"\n",
    "assert norm(final_grad[x>0]) < 1e-2,  \"Your solution is not optimal\"\n",
    "assert min(final_grad[x==0]) >= 0,  \"Your solution is not optimal\"\n",
    "print('TEST PASSED! Wow! You are so in the zone right now!')\n",
    "#good_job(\"https://www.cs.umd.edu/~tomg/img/important_memes/good_job_otter.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem I solve above is solved with accutary 1e-1. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PASSED! Wow! You are so in the zone right now!\n"
     ]
    }
   ],
   "source": [
    "assert min(x) >= 0, \"Your solution does not satisfy the constraint\"\n",
    "assert norm(final_grad[x>0]) < 1e-1,  \"Your solution is not optimal\"\n",
    "assert min(final_grad[x==0]) >= 0,  \"Your solution is not optimal\"\n",
    "print('TEST PASSED! Wow! You are so in the zone right now!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Use your solver on the \"Netflix\" problem.\n",
    "Consider a matrix $X$ of movie rankings with customers on one axis, movies on the other.  In real life our customers have only scored a small number of movies, and we want to recover the entire ranking matrix from a small sample.  If we assume the unknown matrix of movie scores is low-rank, then we can estimate it by solving \n",
    "$$\\arg \\min \\quad  \\mu\\|X\\|_* + \\frac{1}{2}\\|M\\cdot X- B\\|^2$$\n",
    "where $M$ is a mask matrix (with a 1 for scores we observed and 0 for scores we didn't), $\\| \\|_*$ is the nuclear norm, and $\\cdot$ denotes entry-wise multiplication.  \n",
    "\n",
    "Let's start by cooking up a test problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The singular values of the rank-2 matrix are:  [1.48008877e+01 6.94373333e+00 1.60330705e-15 4.67696750e-16\n",
      " 3.52621733e-16 2.11004474e-16 1.91241280e-16 1.25645174e-16\n",
      " 8.26371277e-17 5.03120455e-17]\n",
      "The numerical rank of the matrix is:  2\n"
     ]
    }
   ],
   "source": [
    "# Don't modify this block!\n",
    "n = 10    # How many customers?\n",
    "m = 20    # How many movies?\n",
    "M = randint(low=0, high=2, size=(m,n)) # A mask of 0s and 1s\n",
    "X_true = randn(m,2)@randn(2,n)  # A random rank-2 matrix\n",
    "B = X_true*M   # We only observe scores that are un-masked \n",
    "mu = 2         # A regularization parameter.  Ideally this should be set by cross-validation, but I'll just pick 2.\n",
    "\n",
    "# Let's check that X_true is indeed low rank. Note that I'm using np.linalg.svd.  \n",
    "# Boy, that np.linalg.svd function sure is useful. \n",
    "u, s, vh = np.linalg.svd(X_true, full_matrices=False)\n",
    "print('The singular values of the rank-2 matrix are: ', s) \n",
    "print('The numerical rank of the matrix is: ', np.linalg.matrix_rank(X_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now write your down the components you need to solve this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out the implementations of these methods. I suggest using lambda functions to keep it short\n",
    "f = lambda X: 0.5*norm(M*X-B)**2 \n",
    "gradf = lambda X: M*(M*X-B)\n",
    "g = lambda x: 0   # Don't change this line\n",
    "\n",
    "def proxg(Z,t):\n",
    "    \n",
    "    u, s, vh = np.linalg.svd(Z,full_matrices=False)\n",
    "    d = np.diag(s)\n",
    "    shrink = np.sign(d)*np.fmax(np.abs(d) - mu*t, 0)\n",
    "\n",
    "    return u@shrink@vh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little test never hurt anyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PASSED! Wow! You are so in the zone right now!\n"
     ]
    }
   ],
   "source": [
    "# Do not modify this block!\n",
    "X0 = zeros((m,n))\n",
    "X = fbs(f, gradf, proxg, X0, tol=1e-8) # Note: I use super high accuracy here so that my unit best below works\n",
    "\n",
    "rank = np.linalg.matrix_rank(X, tol=1e-5)\n",
    "#print(rank, norm(X - proxg(X - gradf(X),1)))\n",
    "assert rank <=3, \"Your solution is not low rank\"\n",
    "assert norm(X - proxg(X - gradf(X),1)) < 1e-4,  \"Your solution is not optimal\"\n",
    "print('TEST PASSED! Wow! You are so in the zone right now!')\n",
    "#good_job(\"https://www.cs.umd.edu/~tomg/img/important_memes/otter_news.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
